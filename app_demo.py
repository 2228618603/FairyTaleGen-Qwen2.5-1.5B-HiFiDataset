import gradio as gr
import torch
from threading import Thread
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
from peft import PeftModel

base_model_path = "YOUR_BASE_MODEL_DIR" 
lora_adapter_path = "YOUR_LORA_ADAPTOR__MODEL_PATH"

print("--- æ­£åœ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨, è¯·ç¨å€™... ---")
tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

model = PeftModel.from_pretrained(base_model, lora_adapter_path)
model.eval()

print("--- æ¨¡å‹åŠ è½½å®Œæˆï¼Gradio Web UI å³å°†å¯åŠ¨ã€‚ ---")

def chat_stream(message: str, history: list):
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¼˜ç§€çš„ç«¥è¯æ•…äº‹ä½œå®¶ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„è¦æ±‚åˆ›ä½œä¸€ä¸ªå®Œæ•´ã€æœ‰æ•™è‚²æ„ä¹‰çš„ç«¥è¯æ•…äº‹ã€‚"

    messages = [{"role": "system", "content": system_prompt}]
    for user_msg, ai_msg in history:
        messages.append({"role": "user", "content": user_msg})
        messages.append({"role": "assistant", "content": ai_msg})
    messages.append({"role": "user", "content": message})

    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

    generation_kwargs = dict(
        model_inputs,
        streamer=streamer,
        max_new_tokens=1024,
        do_sample=True,
        top_p=0.9,
        temperature=0.7,
    )

    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()

    generated_text = ""
    for new_text in streamer:
        generated_text += new_text
        yield generated_text

with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown(
        """
        # ğŸ‘‘ æˆ‘çš„ç«¥è¯æ•…äº‹AIä½œå®¶ ğŸ§¸
        
        è¿™æ˜¯åŸºäº Qwen1.5-1.8B æ¨¡å‹é€šè¿‡ LoRA å¾®è°ƒå¾—åˆ°çš„ç«¥è¯æ•…äº‹ç”ŸæˆåŠ©æ‰‹ã€‚
        
        **å°è¯•è¾“å…¥ä¸€ä¸ªä¸»é¢˜ï¼Œæ¯”å¦‚ï¼š**
        - ç»™æˆ‘è®²ä¸€ä¸ªå…³äºâ€œå‹è°Šâ€çš„æ•…äº‹ã€‚
        - å†™ä¸€ä¸ªä¸»è§’æ˜¯å°ç‹ç‹¸ï¼Œä¸»é¢˜æ˜¯â€œè¯šå®â€çš„ç«¥è¯ã€‚
        - æˆ‘æƒ³å¬ä¸€ä¸ªå‘ç”Ÿåœ¨é­”æ³•æ£®æ—é‡Œçš„æ•…äº‹ã€‚
        """
    )
    
    gr.ChatInterface(
        fn=chat_stream,
        title="ç«¥è¯æ•…äº‹AIä½œå®¶",
        examples=[
            ["ç»™æˆ‘è®²ä¸€ä¸ªå…³äº'å‹‡æ•¢'çš„ç«¥è¯æ•…äº‹"],
            ["å†™ä¸€ä¸ªå…³äºå°æ¾é¼ å­¦ä¼š'åˆ†äº«'çš„æ•…äº‹"],
            ["æˆ‘æƒ³å¬ä¸€ä¸ªå…³äºä¿æŠ¤ç¯å¢ƒçš„ç«¥è¯"]
        ],
        chatbot=gr.Chatbot(height=500),
        textbox=gr.Textbox(placeholder="è¯·è¾“å…¥ä½ æƒ³å¬çš„æ•…äº‹ä¸»é¢˜...", container=False, scale=7),
        clear_btn="æ¸…ç©ºå¯¹è¯",
        undo_btn="æ’¤é”€ä¸Šä¸€è½®",
        retry_btn="é‡æ–°ç”Ÿæˆ",
    )

demo.launch(share=True)